package main

import (
	"car-price-prediction/internal/api"
	"car-price-prediction/internal/prediction"
	"log"
	"os"
	"path/filepath"
	"runtime"

	_ "car-price-prediction/docs" // docs is generated by Swag CLI
	onnx "github.com/yalue/onnxruntime_go"
)

// getSharedLibPath returns the path to the ONNX runtime shared library based on the OS and architecture
func getSharedLibPath() string {
	cwd, err := os.Getwd()
	if err != nil {
		log.Fatalf("Failed to get current working directory: %v", err)
	}
	basePath := filepath.Join(cwd, "lib", "third_party")

	libName := ""
	if runtime.GOOS == "windows" {
		if runtime.GOARCH == "amd64" {
			libName = "onnxruntime.dll"
		}
	} else if runtime.GOOS == "darwin" {
		if runtime.GOARCH == "arm64" {
			libName = "onnxruntime_arm64.dylib"
		} else if runtime.GOARCH == "amd64" {
			libName = "onnxruntime_amd64.dylib"
		}
	} else if runtime.GOOS == "linux" {
		if runtime.GOARCH == "arm64" {
			libName = "onnxruntime_arm64.so"
		} else {
			libName = "onnxruntime.so"
		}
	}

	if libName == "" {
		log.Fatalf("Unable to determine a path to the onnxruntime shared library for OS \"%s\" and architecture \"%s\".", runtime.GOOS, runtime.GOARCH)
	}

	return filepath.Join(basePath, libName)
}

// @title Car Price Prediction API
// @version 1.0
// @description This is a sample server for a car price prediction API.

// @host localhost:8080
// @BasePath /
func main() {
	// Define the model path
	modelPath := "model/best_model.onnx"

	// Set the path to the ONNX runtime shared library
	onnx.SetSharedLibraryPath(getSharedLibPath())

	// Initialize the ONNX runtime environment
	// This is required before creating any ONNX sessions
	if err := onnx.InitializeEnvironment(); err != nil {
		log.Fatalf("Failed to initialize ONNX environment: %v", err)
	}
	defer onnx.DestroyEnvironment()

	// Create a new session with the model
	// Note: The onnxruntime library must be installed on the system.
	// For macOS: brew install onnxruntime
	// For Linux: sudo apt-get install libonnxruntime
	// We'll create a simple session without pre-allocating tensors
	// The actual tensors will be created in the Predict method
	// Create input and output tensors for the session
	// We'll create empty tensors with the expected shapes
	// The actual data will be filled in the Predict method
	inputTensor, err := onnx.NewEmptyTensor[float32]([]int64{1, int64(prediction.ModelInputSize)})
	if err != nil {
		log.Fatalf("Failed to create input tensor: %v", err)
	}
	defer inputTensor.Destroy()

	outputTensor, err := onnx.NewEmptyTensor[float32]([]int64{1, 1})
	if err != nil {
		log.Fatalf("Failed to create output tensor: %v", err)
	}
	defer outputTensor.Destroy()

	// Create a session with the model
	// We'll use NewAdvancedSession to specify the input and output tensors
	session, err := onnx.NewAdvancedSession(
		modelPath,
		[]string{"float_input"},  // Correct input tensor name from model inspection
		[]string{"variable"},  // Correct output tensor name from model inspection
		[]onnx.ArbitraryTensor{inputTensor},
		[]onnx.ArbitraryTensor{outputTensor},
		nil,
	)
	if err != nil {
		log.Fatalf("Failed to create ONNX session: %v", err)
	}
	defer session.Destroy()

	// Create a new prediction service with just the model path.
	predictionService := prediction.NewPredictionService(modelPath)

	// Set up the Gin router.
	router := api.SetupRouter(predictionService)

	// Start the server.
	log.Println("Starting server on :8080")
	if err := router.Run(":8080"); err != nil {
		log.Fatalf("Failed to start server: %v", err)
	}
}